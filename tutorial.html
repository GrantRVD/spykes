<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Tutorial &#8212; spykes 0.1 documentation</title>
    
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/gallery.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="&lt;no title&gt;" href="auto_examples/index.html" />
    <link rel="prev" title="Installation" href="installation.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body role="document">
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="tutorial">
<h1>Tutorial<a class="headerlink" href="#tutorial" title="Permalink to this headline">¶</a></h1>
<div class="section" id="fitting-tuning-curves-with-gradient-descent">
<h2>Fitting Tuning Curves with Gradient Descent<a class="headerlink" href="#fitting-tuning-curves-with-gradient-descent" title="Permalink to this headline">¶</a></h2>
<p>The firing rates <span class="math">\(y_j\)</span> of neuron <span class="math">\(j\)</span> can be modeled as a
Poisson random variable.</p>
<div class="math">
\[y_j = \text{Poisson}(\lambda_j)\]</div>
<p>We will drop the subscript <span class="math">\(j\)</span> for convenience of notation and
figure out how to fit the tuning curves of a given neuron <span class="math">\(j\)</span>.</p>
<p>The mean <span class="math">\(\lambda\)</span> is given by the von Mises tuning model as
follows.</p>
<div class="math">
\[\lambda = b + g\exp\Big(\kappa_0 + \kappa \cos(x - \mu)\Big)\]</div>
<p>However, this formulation is non-convex in <span class="math">\(\mu\)</span>. Therefore, we
re-parameterize it to be more tractable (still non-convex in <span class="math">\(b\)</span>
and <span class="math">\(g\)</span>) as follows.</p>
<div class="math">
\[\lambda = b + g\exp\Big(\kappa_0 + \kappa_1 \cos(x) + \kappa_2 \sin(x) \Big),\]</div>
<p>where <span class="math">\(\kappa_1 = \kappa \cos(\mu)\)</span> and
<span class="math">\(\kappa_2 = \kappa \sin(\mu)\)</span>.</p>
<p>Once we estimate <span class="math">\(\kappa_1\)</span> and <span class="math">\(\kappa_2\)</span>, we can back out
<span class="math">\(\kappa\)</span> and <span class="math">\(\mu\)</span> as
<span class="math">\(\kappa = \sqrt{\kappa_1^2 + \kappa_2^2}\)</span>, and
<span class="math">\(\mu = \tan^{-1}\Big(\frac{\kappa_2}{\kappa_1}\Big)\)</span>.</p>
<p>We estimate two special cases of this generalized von Mises model.</p>
<div class="section" id="special-case-1-poisson-generalized-linear-model-glm">
<h3>Special Case 1: Poisson Generalized Linear Model (GLM)<a class="headerlink" href="#special-case-1-poisson-generalized-linear-model-glm" title="Permalink to this headline">¶</a></h3>
<p>If we set <span class="math">\(b = 0\)</span> and <span class="math">\(g =1\)</span>, we get:</p>
<div class="math">
\[\lambda = \exp\Big(\kappa_0 + \kappa_1 \cos(x) + \kappa_2 \sin(x) \Big),\]</div>
<p>This is identical to a Poisson GLM.</p>
<p>The advantage of this formulation is that it is convex and the
disadvantage is that all parameters are not straightforward to
interpret, with <span class="math">\(\kappa_0\)</span> playing the role of both a baseline and
a gain term.</p>
</div>
<div class="section" id="special-case-2-generalized-von-mises-model-gvm">
<h3>Special Case 2: Generalized von Mises Model (GVM)<a class="headerlink" href="#special-case-2-generalized-von-mises-model-gvm" title="Permalink to this headline">¶</a></h3>
<p>If we set <span class="math">\(\kappa_0 = 0\)</span>, we get:</p>
<div class="math">
\[\lambda = b + g\exp\Big(\kappa_1 \cos(x) + \kappa_2 \sin(x) \Big),\]</div>
<p>This is identical to a Eq. (4) in <a class="reference external" href="http://brain.umn.edu/pdfs/BA118.pdf">Amirikan &amp; Georgopulos
(2000)</a>.</p>
<p>The advantage of this formulation is is that although it is non-convex,
we can easily interpret the parameters: - <span class="math">\(b\)</span>, as the baseline
firing rate - <span class="math">\(g\)</span>, as the gain - <span class="math">\(\kappa\)</span>, as the width or
shape</p>
</div>
</div>
<div class="section" id="minimizing-negative-log-likelihood-with-gradient-descent">
<h2>Minimizing Negative Log Likelihood with Gradient Descent<a class="headerlink" href="#minimizing-negative-log-likelihood-with-gradient-descent" title="Permalink to this headline">¶</a></h2>
<p>Given a set of observations <span class="math">\((x_i, y_i)\)</span>, to identify the
parameters
<span class="math">\(\Theta = \left\{\kappa_0, \kappa_1, \kappa_2, g, b\right\}\)</span> we
use gradient descent on the loss function <span class="math">\(J\)</span>, specified by the
negative Poisson log-likelihood,</p>
<div class="math">
\[J = -\log\mathcal{L} = \sum_{i} \lambda_i - y_i \log \lambda_i\]</div>
<p>Taking the gradients, we get:</p>
<div class="math">
\[\frac{\partial J}{\partial \kappa_0} = \sum_{i} g \exp\Big(\kappa_0 + \kappa_1 \cos(x_i) + \kappa_2 \sin(x_i) \Big) \bigg(1 - \frac{y_i}{\lambda_i}\bigg)\]</div>
<div class="math">
\[\frac{\partial J}{\partial \kappa_1} = \sum_{i} g \exp\Big(\kappa_0 + \kappa_1 \cos(x_i) + \kappa_2 \sin(x_i) \Big) \cos(x_i) \bigg(1 - \frac{y_i}{\lambda_i}\bigg)\]</div>
<div class="math">
\[\frac{\partial J}{\partial \kappa_2} = \sum_{i} g \exp\Big(\kappa_0 + \kappa_1 \cos(x_i) + \kappa_2 \sin(x_i) \Big) \sin(x_i) \bigg(1 - \frac{y_i}{\lambda_i}\bigg)\]</div>
<div class="math">
\[\frac{\partial J}{\partial g} = \sum_{i} g \exp\Big(\kappa_0 + \kappa_1 \cos(x_i) + \kappa_2 \sin(x_i) \Big) \bigg(1 - \frac{y_i}{\lambda_i}\bigg)\]</div>
<div class="math">
\[\frac{\partial J}{\partial b} = \sum_{i} \bigg(1 - \frac{y_i}{\lambda_i}\bigg)\]</div>
</div>
<div class="section" id="decoding-feature-from-population-activity">
<h2>Decoding Feature from Population Activity<a class="headerlink" href="#decoding-feature-from-population-activity" title="Permalink to this headline">¶</a></h2>
<p>Under the same Poisson firing rate model for each neuron, whose mean is
specified by the von Mises tuning curve, as above, we can decode the
stimulus <span class="math">\(\hat{x}\)</span> that is most likely to have produced the
observed population activity
<span class="math">\(Y = \left\{y_j, j = 1, 2, \dots \text{n_neurons}\right\}\)</span>.</p>
<p>We will assume that the neurons are conditionally independent given the
tuning parameters <span class="math">\(\Theta\)</span>. Thus the likelihood of observing the
population activity <span class="math">\(Y\)</span> is given by</p>
<div class="math">
\[P(Y | \Theta) = \prod_j P(y_j | \Theta)\]</div>
<p>As before, the loss function for the decoder is given by the negative
Poisson log-likelihood:</p>
<div class="math">
\[J = -\log\mathcal{L} = \sum_j \lambda_j - y_j \log \lambda_j\]</div>
<p>where</p>
<div class="math">
\[\lambda_j = b_j + g_j \exp\Big(\kappa_{0,j} + \kappa_{1,j} \cos(x) + \kappa_{1,j} \sin(x) \Big)\]</div>
<p>To minimize this loss function with gradient descent, we need to take
the gradient of <span class="math">\(J\)</span> with respect to <span class="math">\(x\)</span></p>
<div class="math">
\[\frac{\partial J}{\partial x} = \sum_{j} g_j \exp\Big(\kappa_{0,j} + \kappa_{1,j} \cos(x) + \kappa_{2,j} \sin(x) \Big) \Big(\kappa_{2,j} \cos(x) - \kappa_{1,j} \sin(x)\Big) \bigg(1 - \frac{y_j}{\lambda_j}\bigg)\]</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Tutorial</a><ul>
<li><a class="reference internal" href="#fitting-tuning-curves-with-gradient-descent">Fitting Tuning Curves with Gradient Descent</a><ul>
<li><a class="reference internal" href="#special-case-1-poisson-generalized-linear-model-glm">Special Case 1: Poisson Generalized Linear Model (GLM)</a></li>
<li><a class="reference internal" href="#special-case-2-generalized-von-mises-model-gvm">Special Case 2: Generalized von Mises Model (GVM)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#minimizing-negative-log-likelihood-with-gradient-descent">Minimizing Negative Log Likelihood with Gradient Descent</a></li>
<li><a class="reference internal" href="#decoding-feature-from-population-activity">Decoding Feature from Population Activity</a></li>
</ul>
</li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="installation.html" title="previous chapter">Installation</a></li>
      <li>Next: <a href="auto_examples/index.html" title="next chapter">&lt;no title&gt;</a></li>
  </ul></li>
</ul>
</div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/tutorial.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2016, KordingLab.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.5.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.9</a>
      
      |
      <a href="_sources/tutorial.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>